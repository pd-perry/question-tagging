{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from string import punctuation\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('Xdf/dataset/nus-sms-corpus-master/smsCorpus_zh_2015.03.09.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.DataFrame(columns=['id', 'text', 'label'])\n",
    "count = 0\n",
    "empty = []\n",
    "for tag in root.findall('message/text'):\n",
    "    \n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "        \n",
    "    text_phrase = ET.tostring(tag, encoding='utf8', method='xml').decode('UTF-8', 'strict')[44:-7]\n",
    "    text_phrase = text_phrase.replace(' ','')\n",
    "    sentence = []\n",
    "\n",
    "    tem = re.split(r'\\s+|[?？]\\s*', text_phrase)\n",
    "\n",
    "    #loops through list after question mark\n",
    "    for i in range(len(tem)):\n",
    "        #add question mark to every element\n",
    "        if tem[i] != tem[-1]:\n",
    "            tem[i] += '?'\n",
    "        sentence += list(filter(lambda x: x != \"?\" and x != \"\", re.split(r'\\s+|[.。!！]\\s*', tem[i])))\n",
    "\n",
    "    for sen in sentence:\n",
    "        if '？' in sen or '?' in sen:\n",
    "            raw.loc[count] = [count, re.sub(r'\\s+|[，,]\\s*', '', sen[:-1]), 1]\n",
    "        else:\n",
    "            raw.loc[count] = [count, re.sub(r'\\s+|[，,]\\s*', '', sen), 0]\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.to_csv('nusconv_nopunc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation and Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.read_csv('rawdata/nusconv_nopunc.csv')\n",
    "processed.drop([38848], inplace=True)\n",
    "stopwords = pd.read_csv('cn_stopwords.txt').iloc[:, 0].to_list()\n",
    "\n",
    "list_of_text = processed['text'].to_list()\n",
    "\n",
    "for i in range(len(list_of_text)):\n",
    "    segmented = ','.join(jieba.cut(list_of_text[i]))\n",
    "    segmented = segmented.split(',')\n",
    "    rsw = ''\n",
    "    for word in segmented:\n",
    "        if word not in stopwords:\n",
    "            rsw += word\n",
    "            rsw += ' '\n",
    "    processed.loc[i, 'text'] = rsw\n",
    "\n",
    "processed = processed[['text', 'label']]\n",
    "processed.loc[38848, 'label'] = 0.0\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = processed['text'].to_list()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X =  vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names)\n",
    "print(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words():\n",
    "    '''\n",
    "    doc is each document in numpy string format\n",
    "    corpus is a list of sentences each contained in a seperate list\n",
    "    '''\n",
    "    corpus = []\n",
    "    \n",
    "    for doc_num in range(100):\n",
    "        if doc_num < 10:\n",
    "            doc = np.genfromtxt('rawdata/wikipedia/wiki_0' + str(doc_num) + '.txt', dtype=str, delimiter='\\n')\n",
    "        elif doc_num >= 10 and doc_num < 33:\n",
    "            doc = np.genfromtxt('rawdata/wikipedia/wiki_' + str(doc_num) + '.txt', dtype=str, delimiter='\\n')\n",
    "        elif doc_num >= 33:\n",
    "            doc = np.genfromtxt('rawdata/wikipedia/wiki_' + str(doc_num), dtype=str, delimiter='\\n')\n",
    "        for i in range(len(doc)):\n",
    "            try:\n",
    "                for j in re.split(r'\\s+|[.。!！?？]\\s*', json.loads(doc[i])['text']):\n",
    "                    corpus += [list(jieba.cut(re.sub(r'\\s+|[，,、]\\s*', '', j)))]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "senten = processed['text'].to_list()\n",
    "spl = [list(i.split()) for i in senten] + get_words()\n",
    "model = gensim.models.Word2Vec(spl, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_original = pd.read_csv('rawdata/nusconv_nopunc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = [0.0] * 100\n",
    "'''\n",
    "lst = [not np.array_equal(vectorized.text[i], empty) for i in range(len(vectorized))]\n",
    "check = pd.DataFrame(lst, columns=['text'])\n",
    "\n",
    "vectorized_rm = vectorized[check]\n",
    "'''\n",
    "\n",
    "processed = processed[processed.text != '']\n",
    "processed.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not efficient\n",
    "\n",
    "processed_originalcut = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "processed_originalcut = processed_original[processed_original.id.isin(processed['index'].to_list())]\n",
    "\n",
    "processed_originalcut.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "'''\n",
    "count = 0\n",
    "\n",
    "for i in processed['index']:\n",
    "    processed_originalcut.loc[count, :] = processed_original.loc[i, :]\n",
    "    count += 1\n",
    "'''\n",
    "\n",
    "print(processed_originalcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = pd.DataFrame(columns=['text', 'label'])\n",
    "count = 0\n",
    "for i in processed['text']:\n",
    "    vec = np.zeros([1, 100])\n",
    "    for j in list(i.split()):\n",
    "        vec += model[j]\n",
    "    vectorized.loc[count, 'text'] = vec[0]\n",
    "    vectorized.loc[count, 'label'] = processed.loc[count, 'label']\n",
    "    count += 1\n",
    "    \n",
    "print(vectorized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['不', '没有'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = vectorized.loc[vectorized['label'] == 1.0]\n",
    "pos_text = processed.loc[vectorized['label'] == 1.0]\n",
    "pto = processed_originalcut.loc[vectorized['label'] == 1.0]\n",
    "\n",
    "neg = vectorized.loc[vectorized['label'] == 0.0][0:6000]\n",
    "neg_text = processed.loc[vectorized['label'] == 0.0][0:6000]\n",
    "nto = processed_originalcut.loc[vectorized['label'] == 0.0][0:6000]\n",
    "\n",
    "resampled = pos.append(neg)\n",
    "resampled_text = pos_text.append(neg_text)\n",
    "rto = pto.append(nto)\n",
    "\n",
    "shuffled = resampled.sample(n=12000, random_state=1)\n",
    "shuffled_text = resampled_text.sample(n=12000, random_state=1)\n",
    "sto = rto.sample(n=12000, random_state=1)\n",
    "\n",
    "shuffled.reset_index(inplace = True)\n",
    "shuffled_text.reset_index(inplace = True)\n",
    "sto.reset_index(inplace=True)\n",
    "\n",
    "x_train = shuffled.loc[0:10000, 'text'].to_list()\n",
    "y_train = shuffled.loc[0:10000, 'label'].to_list()\n",
    "\n",
    "x_test = shuffled.loc[10000:, 'text'].to_list()\n",
    "y_test = shuffled.loc[10000:, 'label'].to_list()\n",
    "\n",
    "xtt = shuffled_text.loc[10000:, 'text'].to_list()\n",
    "\n",
    "xot = sto.loc[10000:, 'text'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "fpr, tpr, threshhold = roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred))\n",
    "print(\"auc: \", auc(fpr, tpr))\n",
    "\n",
    "#floor = c < 0.5\n",
    "#c[floor] = 0\n",
    "#ceiling = c >= 0.5\n",
    "#c[ceiling] = 1\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict((model['']).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = pd.DataFrame(columns=[\"Index\", \"Original_Text\", \"Processed_Text\", \"Predicted\", \"Labels\"])\n",
    "\n",
    "count = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        wrong.loc[count, :] = i, xot[i], xtt[i].strip(), y_pred[i], y_test[i]\n",
    "        count += 1\n",
    "        #print('Index: {},\\nText: {},\\nPredicted: {},\\nLabel: {}\\n'.format(i, xtt[i].strip(), y_pred[i], y_test[i]))\n",
    "\n",
    "\n",
    "print(wrong.loc[570:590, :])\n",
    "wrong.to_csv('wrong3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve, confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "print(confusion_matrix)\n",
    "disp = plot_confusion_matrix(clf, x_test, y_test,\n",
    "                                 display_labels=['Non Question','Question'],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "disp.ax_.set_title('confusion_matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "#clf_display = plot_roc_curve(clf, x_train, y_train, ax=ax, alpha=0.8)\n",
    "svc_display = plot_roc_curve(clf, x_train, y_train)\n",
    "svc_display.plot(ax=ax, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.to_csv('nusprocessed.csv')\n",
    "vectorized.to_csv('nusvectorized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a mesh to plot in\n",
    "\n",
    "h = 0.02\n",
    "\n",
    "x_min, x_max = np.amin(x_train) - 1, np.amax(x_train) + 1\n",
    "y_min, y_max = np.amin(y_train) - 1, np.amax(y_train) + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title(' ')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
