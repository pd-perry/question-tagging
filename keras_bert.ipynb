{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import keras\n",
    "from keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from collections import namedtuple\n",
    "from keras.utils.np_utils import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "config =tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.log_device_placement = True\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess =tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras_bert import Tokenizer\n",
    "pretrained_path  = \"bert/\"\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 120\n",
    "MAX_LEN = 384\n",
    "MODEL_NAME = 'bert_model.h5'\n",
    "PretrainedPaths = namedtuple('PretrainedPaths', ['config', 'model', 'vocab'])\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "model_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "paths = PretrainedPaths(config_path, model_path, vocab_path)\n",
    "#tokenizer = Tokenizer(paths.vocab)\n",
    "from keras_bert import load_vocabulary\n",
    "token_dict = load_vocabulary(vocab_path)\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "text = '语言模型'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "indices, segments = tokenizer.encode(first=text, max_len=120)\n",
    "print(indices[:10])\n",
    "# [101, 6427, 6241, 3563, 1798, 102, 0, 0, 0, 0]\n",
    "print(segments[:10])\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert.layers import Extract\n",
    "model = load_trained_model_from_checkpoint(\n",
    "config_file=paths.config,\n",
    "checkpoint_file=paths.model,\n",
    "seq_len = MAX_LEN,\n",
    "trainable = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = model.output\n",
    "extract = Extract(index=-1, name='Extract')(last)\n",
    "dense = keras.layers.Dense(units=768, name='Dense')(extract)\n",
    "norm = keras.layers.BatchNormalization(name='Normal')(dense)\n",
    "output = keras.layers.Dense(units=2, activation='softmax', name='Softmax')(norm)\n",
    "model = keras.models.Model(inputs=model.inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(full_x, full_y,tokenizer, batch_size, num):\n",
    "    while 1:\n",
    "        cnt = 0\n",
    "        ind = []\n",
    "        seg = []\n",
    "        mas = []\n",
    "        Y = []\n",
    "        for i in range(num):\n",
    "            try:\n",
    "                indices,segments = tokenizer.encode(first=full_x[i],max_len=MAX_LEN)\n",
    "            #masked = [1] * len(full_x[i]) + [0] * (max_len-len(full_x[i]))\n",
    "                ind.append(indices)\n",
    "                seg.append(segments)\n",
    "                Y.append(full_y[i])\n",
    "                cnt += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "            if cnt == batch_size:\n",
    "                cnt = 0\n",
    "                Y = to_categorical(Y, 2) ## one hot 编码\n",
    "                yield ([np.array(ind),np.array(seg)], np.array(Y))\n",
    "                ind = []\n",
    "                seg = []\n",
    "                mas = []\n",
    "                Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.dropna(inplace=True)\n",
    "    data = data[data.text != '']\n",
    "    \n",
    "    pos = data.loc[data['label'] == 1.0]\n",
    "    neg = data.loc[data['label'] == 0.0]\n",
    "    \n",
    "    num_data = min(len(pos), len(neg)) #number of samples for each label\n",
    "    \n",
    "    pos = pos[0: num_data]\n",
    "    neg = neg[0: num_data]\n",
    "\n",
    "    resampled = pos.append(neg)\n",
    "    \n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nusconv = pd.read_csv('nusconv_nopunc.csv')\n",
    "chatbot = pd.read_csv('chatbot.csv')\n",
    "\n",
    "nus_conv = preprocess(nusconv)\n",
    "chat_bot = preprocess(chatbot)\n",
    "\n",
    "shuffled = nus_conv.append(chat_bot)\n",
    "shuffled = shuffled.sample(len(shuffled), random_state=1)\n",
    "shuffled.reset_index(inplace=True)\n",
    "shuffled = shuffled.loc[:, ['text', 'label']]\n",
    "\n",
    "print(shuffled)\n",
    "\n",
    "train_x = shuffled.loc[0:12000, 'text'].to_list()\n",
    "train_y = shuffled.loc[0:12000, 'label'].to_list()\n",
    "\n",
    "val_x = shuffled.loc[12000:, 'text'].to_list()\n",
    "val_y = shuffled.loc[12000:, 'label'].to_list()\n",
    "\n",
    "\n",
    "\n",
    "print(len([y for y in val_y if y == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "#del model\n",
    "model = load_model('bert/bert_model_0525.h5', custom_objects=get_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    generator=get_batch(train_x, train_y,tokenizer, BATCH_SIZE, len(train_x)),\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=1,\n",
    "    validation_data=get_batch(val_x, val_y,tokenizer, BATCH_SIZE, len(val_x)),\n",
    "    validation_steps=100,\n",
    "    callbacks=[\n",
    "        #keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "       keras.callbacks.ModelCheckpoint('bert/model_.h5', monitor='val_acc', period=1, verbose=1, save_best_only=True, mode='max')\n",
    "    ],\n",
    ")\n",
    "#keras.callbacks.ModelCheckpoint('model_.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "test = pd.read_csv('rawdata/ywj.csv')\n",
    "test = test.iloc[:, 0:2]\n",
    "tt = pd.read_csv('rawdata/csj.csv')\n",
    "tt = tt.iloc[:, 0:2]\n",
    "test['label'] = 0\n",
    "sentence = []\n",
    "count = 0\n",
    "for example in test['疑问句'].to_list():\n",
    "    if example[-1] == '？' or example[-1] == '?':\n",
    "        test.loc[count] = [count, re.sub(r'\\s+|[，,.。?？]\\s*', '', example[:-1]), 1]\n",
    "    else:\n",
    "        test.loc[count] = [count, re.sub(r'\\s+|[，,.。?？]\\s*', '', example), 1]\n",
    "    count += 1\n",
    "for example in tt['陈述句'].to_list():\n",
    "    if example[-1] == '？' or example[-1] == '?':\n",
    "        test.loc[count] = [count, re.sub(r'\\s+|[，,.。?？]\\s*', '', example[:-1]), 0]\n",
    "    else:\n",
    "        test.loc[count] = [count, re.sub(r'\\s+|[，,.。?？]\\s*', '', example), 0]\n",
    "    count += 1\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize(test_data_x):\n",
    "    ind = []\n",
    "    seg = []\n",
    "    mas = []\n",
    "    for i in range(len(test_data_x)):\n",
    "        try:\n",
    "            indices,segments = tokenizer.encode(first=test_data_x[i],max_len=MAX_LEN)\n",
    "            ind.append(indices)\n",
    "            seg.append(segments)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return [np.array(ind),np.array(seg)]\n",
    "\n",
    "x_test = test_tokenize(test['疑问句'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "%reload_ext memory_profiler\n",
    "\n",
    "@profile\n",
    "def memory_test():\n",
    "    return model.predict(x_test)\n",
    "\n",
    "memory_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import LogFile\n",
    "import sys\n",
    "sys.stdout = LogFile('memory_profile_log')\n",
    "print(sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpred = model.predict(x_test)\n",
    "y_test = test['label'].to_list()\n",
    "rpred = pd.DataFrame(rpred)\n",
    "rpred = rpred.iloc[:, 0]\n",
    "\n",
    "pred = rpred.copy()\n",
    "\n",
    "print(rpred)\n",
    "pred[pred >= 0.5] = 0\n",
    "pred[pred != 0] = 1\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve\n",
    "print(\"Accuracy: \", accuracy_score(y_test, pred.to_list()))\n",
    "print(\"Precision: \", precision_score(y_test, pred))\n",
    "print(\"Recall: \", recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bert/bert_model_0527.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "\n",
    "plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve, confusion_matrix, plot_confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "\n",
    "plt.title = 'Model Loss'\n",
    "plt.ylabel = 'Loss'\n",
    "plt.xlabel = 'Epoch'\n",
    "\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['accuracy'])\n",
    "plt.plot(model.history.history['val_accuracy'])\n",
    "\n",
    "plt.title = 'Model Accuracy'\n",
    "plt.ylabel = 'Accuracy'\n",
    "plt.xlabel = 'Epoch'\n",
    "\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['auc_2'])\n",
    "plt.plot(model.history.history['val_auc_2'])\n",
    "\n",
    "plt.title = 'Model AUC'\n",
    "plt.ylabel = 'AUC'\n",
    "plt.xlabel = 'Epoch'\n",
    "\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred, labels=[0, 1])\n",
    "print(cm)\n",
    "\n",
    "df_confusion = pd.DataFrame(cm, index = [0, 1], columns = [0, 1])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_confusion, annot=True, cmap=\"Blues\", fmt=\"g\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
